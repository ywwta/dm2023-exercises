{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 10 - Embeddings and Graph Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Theoretical questions for Graph Convolutional Networks\n",
    "1. In Graph Convolutional Networks (GCN), what is the feature/attribute matrix $X$ if the graph has no attributes? \n",
    "2. Describe what happens when we add an extra layer to a GCN.\n",
    "3. How many layers should a network have in order for any node to receive \"messages\" from all other nodes? \n",
    "4. Why do we most often use $\\tilde{\\mathbf{A}} = \\mathbf{A} + \\mathbf{I}$ and not just $\\mathbf{A}$ in graph convolutions?\n",
    "4. How can we extend GCNs to learn from multiple graphs $G_1$, ..., $G_r$, at the same time? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Falling in love with PyTorch\n",
    "This exercise is mainly an introduction for those of you who didn't work with PyTorch before (and a recap for the rest of you).\n",
    "\n",
    "PyTorch is a framework similar to Numpy but with more \"muscles.\" The main reason why we wish to use it here is that it has an efficient implementation of [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation), i.e., we can define a \"forward\" computation and PyTorch can compute the gradients for us.\n",
    "\n",
    "We will practice a bit here by making a tiny Neural Network, which can classify MNIST digits.\n",
    "\n",
    "However, first we need to install `pytorch` (and `torchvision`). You can either go to your anaconda console (with `dm20` activated) and run\n",
    "\n",
    "```bash\n",
    "> conda install -c pytorch pytorch torchvision\n",
    "> conda install -c conda-forge ipywidgets\n",
    "```\n",
    "\n",
    "or you can try to run the following code cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install torch torchvision ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then all the imports that we will need this week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch support computation on GPUs, which is much faster than CPUs.\n",
    "So if your machine has a GPU available, we will use that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we have a GPU, we can compute things on it to speed things up. \n",
    "# It might require some time to get it working. You might also need to update drivers for your GPU\n",
    "# Hint: On google colab you can change runtime environment to use GPU. If you need to try some deep learning that requires powerful hardware, \n",
    "# this can be an excelent place to start.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> Below, we have implemented a simple Neural Network for MNIST classification. The only purpose of this part is, as mentioned, to remind you how PyTorch works.\n",
    "> If you are familiar with PyTorch, you can safely skip the remainder of this exercise.\n",
    "\n",
    "For those of you who are not familiar with pytorch, we recommend that you pay particular attention to (and maybe look up documentation for) the `Net(nn.Module)` class and the internals of the training loop. The rest of the code is mainly boilerplate code. \n",
    "\n",
    "We start by loading some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# MNIST dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root='../utilities/data', train=True,  transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='../utilities/data', train=False, transform=transforms.ToTensor())\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then build a tiny Neural Network. Note how we only define layers and forward computations of the network and not any gradient computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=(3, 3), padding=1)\n",
    "        self.max_pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=(3, 3), padding=1)\n",
    "        self.max_pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(32*7*7, 32)\n",
    "        self.fc2 = nn.Linear(32, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutions\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.max_pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.max_pool2(x)\n",
    "        \n",
    "        x = x.view(-1, 32 * 7 * 7)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "    \n",
    "model = Net()\n",
    "# Move neural network to selected device\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(loader=test_loader, max_items=10000):\n",
    "    \"\"\"\n",
    "        This function just runs over the elements of the `loader`\n",
    "        and uses the model to do predictions for accuracy estimates.\n",
    "    \"\"\"\n",
    "    with torch.no_grad(): # We don't need gradients for this\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in loader:\n",
    "            images       = images.to(device)\n",
    "            labels       = labels.to(device)\n",
    "            outputs      = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total       += labels.size(0)\n",
    "            correct     += (predicted == labels).sum().item()\n",
    "            \n",
    "            if total > max_items: break\n",
    "\n",
    "        return 100. * correct /  total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can define a training loop, which will do gradient descent as we are used to but automatic.\n",
    "For this, we need to define an optimizer (the gradient descent functionality) and we need to define a loss function (cross-entropy for class prediction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer (Adam og Stochastic Gradient Descent) and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # Adam works way better than SGD for this particular exmaple.\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# The actual training loop\n",
    "epochs     = 3\n",
    "total_step = len(train_loader)\n",
    "\n",
    "for e in range(epochs): # three epochs\n",
    "    print ('\\rEpoch [%2d/%d] accuracy - train: %.4f \\t test: %.4f' % (e, epochs, test_model(train_loader), test_model(test_loader)))\n",
    "    \n",
    "    # Test the model performance on the test set\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Move tensors to the configured device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss    = loss_function(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad() # Remove gradients from last iteration\n",
    "        loss.backward()       # Compute new gradients\n",
    "        optimizer.step()      # Tune parameters from gradients\n",
    "\n",
    "        if (i+1) % 10 == 0:\n",
    "            print ('\\rEpoch [%2d/%d], Step [%3d/%d], Loss: %.4f' % (e+1, epochs, i+1, total_step, loss.item()), end=\"\")\n",
    "    print()\n",
    "\n",
    "print ('\\rEpoch [%2d/%d] accuracy - train: %.4f \\t test: %.4f' % (epochs, epochs, test_model(train_loader), test_model(test_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having gone through the code, it should be obvious that PyTorch is a very powerful tool if you want to\n",
    "train a model with gradient descent. You basically need to do the following steps and then you are good\n",
    "to go. \n",
    "\n",
    "1. Define your model (subclass `torch.nn.Module`) and the associated forward pass (`def forward(...)`)\n",
    "2. Define your loss (which is differentiable)\n",
    "3. Compute the loss for your data: `loss = loss_fn(y, model(x))`\n",
    "4. Compute gradients and optimize parameters: \n",
    "```python\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3 - Programming a Graph Convolutional Network\n",
    "\n",
    "Now that we know how PyTorch works, we can utilize it to make a Graph Convolutional Network (GCN).\n",
    "\n",
    "In the following, we will i) define a GCN layer in PyTorch ii) Compose a two-layer GCN and iii) Train the network in a supervised fashion using cross-entropy and only two labeled nodes.\n",
    "\n",
    "The setup is the following. We will define a single layer of the GCN as follows:\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\tilde{\\mathbf{A}}&= \\mathbf{A} + \\mathbf{I}\\\\\n",
    "    \\mathbf{D}_{i i}&=\\sum_{j} \\tilde{\\mathbf{A}}_{i, j} \\\\\n",
    "    \\mathbf{L} &= \\mathbf{D}^{-\\frac{1}{2}} \\tilde{\\mathbf{A}} \\mathbf{D}^{-\\frac{1}{2}} \\\\\n",
    "    \\mathbf{H}^0 &= \\mathbf{X}\\\\\n",
    "    \\mathbf{H}^{(k+1)} &=\\sigma\\left(\\mathbf{L} \\mathbf{H}^{(k)} \\mathbf{W}^{(k)} + \\mathbf{b}^{(k)}\\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We can utilize the linear layer `torch.nn.Linear` to make these layers - but how?\n",
    "Below you need to fill in what should happen for a GCN layer.\n",
    "Hint: try to use the `nn.Linear` as a part of your implementation of the GCN layer below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define just one GCN layer\n",
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, L, input_features, output_features, activation=F.relu):\n",
    "        \"\"\"\n",
    "            Inputs:\n",
    "                L:               The \"Laplacian\" of the graph, as defined above\n",
    "                input_features:  The size of the input embedding\n",
    "                output_features: The size of the output embedding \n",
    "                activation:      Activation function sigma\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        ### TODO Your code here\n",
    "        ### TODO Your code here\n",
    "     \n",
    "    def forward(self, X):\n",
    "        ### TODO Your code here\n",
    "        ### TODO Your code here\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having defined a graph convolutinal layer, we can now stack a couple of them to get a GCN.\n",
    "But first, we will need two other ingredients: :\n",
    "1. Implement the computation of the graph Laplacian for our favorite graph. Surprise, surprise - the karate club graph.\n",
    "2. Construct the input feature vectors $X \\in \\mathbb{R}^{n \\times n}$.  \n",
    "    _Hint_: The karate club graph only have one attribute, which is the one we are going to predict, i.e., no features left as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjacency matrix\n",
    "G     = nx.karate_club_graph()\n",
    "A     = np.array(nx.adj_matrix(G, weight=None).todense())\n",
    "I     = np.eye(A.shape[0])\n",
    "A     = A + I # Add self loop\n",
    "\n",
    "### TODO your code here\n",
    "# Degree matrix\n",
    "# Normalized Laplacian\n",
    "# Create input vectors\n",
    "### TODO your code here\n",
    "\n",
    "X = torch.tensor(X, dtype=torch.float, requires_grad=True) # Indicate to pytorch that we need gradients for this variable\n",
    "L = torch.tensor(L, dtype=torch.float)  # We don't need to learn this so no grad required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nodes of the Karate Club graph has one label, namely the club association of each node (member).\n",
    "If you want a bit more detail: \n",
    "\n",
    "> A social network of a karate club was studied by Wayne W. Zachary for a period of three years from 1970 to 1972. The network captures 34 members of a karate club, documenting links between pairs of members who interacted outside the club. During the study a conflict arose between the administrator \"John A\" and instructor \"Mr. Hi\" (pseudonyms), which led to the split of the club into two. Half of the members formed a new club around Mr. Hi; members from the other part found a new instructor or gave up karate. Based on collected data Zachary correctly assigned all but one member of the club to the groups they actually joined after the split.  \n",
    "> Cite: [Wikipedia](https://en.wikipedia.org/wiki/Zachary%27s_karate_club#Network_description)\n",
    "\n",
    "Below, we split the nodes into \"Mr. Hi\" nodes and the rest and treat these as labels for a supervised learning setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ground-truth labels\n",
    "labels = [(0 if d['club']=='Mr. Hi' else 1) for i,d in G.nodes().data()]\n",
    "labels = torch.tensor(labels, dtype=torch.float)\n",
    "y      = labels.to(device)\n",
    "\n",
    "# Constants used for training in a bit. \n",
    "# We are actually only going to use Mr. Hi and the admin for training.\n",
    "ID_INSTR = 0                                       # The Mr. Hi node\n",
    "ID_ADMIN = 33                                      # The Admin  node\n",
    "ID_MEMBERS = set(G.nodes()) - {ID_ADMIN, ID_INSTR} # The rest\n",
    "labels[ID_ADMIN] = 0.75\n",
    "labels[ID_INSTR] = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can finally define our GCN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define convolutional network\n",
    "in_features, out_features = X.shape[1], 2 # two output features\n",
    "hidden_dim = 10\n",
    "\n",
    "# Stack two GCN layers as our model\n",
    "# nn.Sequential is an implicit nn.Module, which uses the layers in given order as the forward pass\n",
    "gcn = nn.Sequential(\n",
    "    GCNLayer(L, in_features, hidden_dim),\n",
    "    GCNLayer(L, hidden_dim, out_features, None),\n",
    "    nn.LogSoftmax(dim=1)\n",
    ")\n",
    "gcn.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train our model. We are going to train on only the Mr. Hi and the Admin nodes.\n",
    "The rest of the nodes are going to be left for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_node_classifier(model, optimizer, X, y, epochs=60, print_every=10):\n",
    "    for epoch in range(epochs+1):\n",
    "        y_pred = model(X)\n",
    "\n",
    "        # Semi-supervised: only use labels of the Instructor and Admin nodes\n",
    "        labelled_idx = [ID_ADMIN, ID_INSTR]\n",
    "        loss = F.nll_loss(y_pred[labelled_idx], y[labelled_idx])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print(f'Epoch {epoch:2d}, loss={loss.item():.5f}')\n",
    "\n",
    "optimizer = torch.optim.Adam(gcn.parameters(), lr=0.01)\n",
    "train_node_classifier(gcn, optimizer, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having trained the network. Let's do a quick evaluation of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = torch.argmax(gcn(X), dim=1).numpy()\n",
    "print(classification_report(y.numpy(), y_pred, target_names=['I','A']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we remove the top most Softmax layer and plot the learned embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_karate(G, pr=[], ax=None, cmap=plt.get_cmap('RdYlBu'), **kwargs): \n",
    "    fixed_positions = {0:(10.74,4.07),1:(9.76,6.48),2:(8.39,5.21),3:(10.37,1.98),4:(12.30,5.61),5:(13.31,3.28),6:(13.28,5.00),7:(8.41,7.06),8:(6.72,4.31),9:(5.77,1.38),10:(12.30,2.72),11:(12.75,4.05),12:(11.32,2.41),13:(8.70,2.88),14:(3.33,0.63),15:(1.88,2.01),16:(13.92,4.05),17:(10.77,5.61),18:(0.69,6.40),19:(9.05,1.38),20:(0.34,4.63),21:(11.56,6.22),22:(5.24,0.34),23:(1.88,7.49),24:(5.11,6.80),25:(4.31,8.52),26:(2.14,0.32),27:(3.65,6.64),28:(6.03,5.24),29:(0.77,2.91),30:(7.01,2.43),31:(6.61,7.86),32:(4.60,4.52),33:(4.39,2.91)}\n",
    "    if len(pr) :\n",
    "        nx.draw(G, with_labels=True, pos=fixed_positions, ax=ax, cmap=cmap, node_color=pr, **kwargs)\n",
    "    else : \n",
    "        nx.draw(G, with_labels=True, pos=fixed_positions, ax=ax, **kwargs)\n",
    "\n",
    "# Visualize the Karate Club graph\n",
    "fig, ax = plt.subplots(1,3, figsize=(14,6))\n",
    "gcn_no_softmax = torch.nn.Sequential(*(list(gcn.children())[:-1])) # Remove the softmax for better plotting\n",
    "node_colors = gcn_no_softmax(X).detach().cpu().numpy()\n",
    "\n",
    "node_labels = {i: i for i in ID_MEMBERS}\n",
    "node_labels.update({i: l for i,l in zip([ID_ADMIN, ID_INSTR],['A','Mr.'])})\n",
    "ax[0].set_title(\"Embedding dim1\")\n",
    "plot_karate(G, node_colors[:,0], ax=ax[0], cmap=plt.get_cmap('coolwarm'), labels=node_labels)\n",
    "ax[1].set_title(\"Embedding dim2\")\n",
    "plot_karate(G, node_colors[:,1], ax=ax[1], cmap=plt.get_cmap('coolwarm'), labels=node_labels)\n",
    "ax[2].set_title(\"Embedding\")\n",
    "# nx.draw(G, pos=node_colors, ax=ax[2], node_color=labels, cmap=plt.get_cmap('coolwarm'), node_size=100)\n",
    "ax[2].scatter(node_colors[:,0], node_colors[:,1], c=labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could try to learn a similar embedding with a standard Neural Network and the same input. This is done in the optional exercises. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "\n",
    "In the previous exercise, we learned the parameters of our Neural Network in a supervised setting (we used cross-entropy).\n",
    "This course is, however, more about unsupervised methods. \n",
    "Let's try to get rid of the labels: \n",
    "\n",
    "1. Make a new GCN, and train it in an unsupervised manner. You can use a setup similar to last week, i.e., Minimize $\\|ZZ^T - S\\|^2_F$, where $Z = GCN(X)$ and $S$ is some similarity matrix.\n",
    "2. Plot the embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "# 1. Definde your model\n",
    "# gcn2 = ...\n",
    "# Stack two GCN layers as our model\n",
    "\n",
    "# 2. Compute some similarity matrix\n",
    "# S = ...\n",
    "def pagerank_matrix(G, alpha = 0.85) :     \n",
    "    \n",
    "    #P = ...\n",
    "    return P\n",
    "Pr = torch.tensor(pagerank_matrix(G)).to(device)\n",
    "### YOUR CODE HERE\n",
    "\n",
    "gcn2 = gcn2.to(device)\n",
    "S    = torch.tensor(S).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(gcn2.parameters(), lr=0.01)\n",
    "epochs = 200\n",
    "\n",
    "for e in range(epochs):\n",
    "    ### YOUR CODE HERE\n",
    "    #out   = gcn2(X)\n",
    "    #sim   = ...\n",
    "    #loss  = ...\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if e % 40 == 0:\n",
    "        print(f'Epoch {e:2d}, loss={loss.item():.5f}')\n",
    "\n",
    "print(f'Epoch {e:2d}, loss={loss.item():.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Karate Club graph\n",
    "fig, ax = plt.subplots(1,3, figsize=(14,6))\n",
    "node_colors = gcn2(X).detach().cpu().numpy()\n",
    "\n",
    "node_labels = {i: i for i in ID_MEMBERS}\n",
    "node_labels.update({i: l for i,l in zip([ID_ADMIN, ID_INSTR],['A','Mr.'])})\n",
    "ax[0].set_title(\"Embedding dim1\")\n",
    "plot_karate(G, node_colors[:,0], ax=ax[0], cmap=plt.get_cmap('coolwarm'), labels=node_labels)\n",
    "ax[1].set_title(\"Embedding dim2\")\n",
    "plot_karate(G, node_colors[:,1], ax=ax[1], cmap=plt.get_cmap('coolwarm'), labels=node_labels)\n",
    "ax[2].set_title(\"Embedding\")\n",
    "# nx.draw(G, pos=node_colors, ax=ax[2], node_color=labels, cmap=plt.get_cmap('coolwarm'), node_size=100)\n",
    "ax[2].scatter(node_colors[:,0], node_colors[:,1], c=labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: \"Deriving\" NetMF Algorithms\n",
    "In class, we saw (on a high level) that DeepWalk asymptoticaly (and implicitly) corresponds to factorizing the matrix \n",
    "\n",
    "$$\n",
    "S = \\log \\left(\\frac{\\operatorname{vol}(G)}{b}\\left(\\frac{1}{T} \\sum_{r=1}^{T}\\left(D^{-1} A\\right)^{r}\\right) D^{-1}\\right), \\qquad \\qquad (1)\n",
    "$$\n",
    "\n",
    "when the lenght of the random walks tends to infinity. Note that $S$ is symmetric and so the SVD of $S$ will be\n",
    "$$\n",
    "   S = U \\Sigma U^T\n",
    "$$\n",
    "due to [this](https://en.wikipedia.org/wiki/Singular_value_decomposition#Relation_to_eigenvalue_decomposition) relationship.\n",
    "So now we could do the best linear rank $d$ approximation of $S$ by the SVD if we wanted to - remember result from last week.\n",
    "\n",
    "We could use this knowledge to do the factorization directly, i.e., compute $S$ from Equation (1) and then use SVD to factorize it.\n",
    "This is going to be our first step before analyzing Equation in more detail.\n",
    "\n",
    "1. Implement the matrix factorization as follows:\n",
    "    1. Compute $M = \\frac{vol(G)}{bT}\\left(\\sum_{r=1}^T (D^{-1}A)^r\\right) D^{-1}$ \n",
    "    2. Compute $M' = \\log( \\operatorname{max}(M, 1))$\n",
    "    3. Decompose $M'$ with the SVD: $M' = U \\Sigma V^T$\n",
    "    4. Compute embedding $Z = U_d \\sqrt{\\Sigma_d}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.karate_club_graph()\n",
    "A = np.array(nx.adjacency_matrix(G).todense())\n",
    "vol_G  = G.number_of_edges()\n",
    "b = 3\n",
    "T = 10\n",
    "\n",
    "### TODO Your code here\n",
    "\n",
    "M = ... \n",
    "M_ = ...\n",
    "U, s, V = ...\n",
    "Z = ...\n",
    "\n",
    "### TODO Your code here\n",
    "\n",
    "Z = Z[:, :2] # Use only the two-dimensional embeddings\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.scatter(*(Z.T), c=labels)\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is great! Now we have yet another graph embedding technique - which is heavily based on theory. \n",
    "We will discover a bit more of the theory here.\n",
    "\n",
    "2. Consider an undirected (weighted) connected graph. Prove that $D^{-\\frac{1}{2}} A D^{-\\frac{1}{2}}$ is symmetric and has eigenvalues in the range $[-1, 1]$.  \n",
    "    _Hint:_ Recall that the normalized Laplacian $\\mathcal{L} = I - D^{-\\frac{1}{2}} A D^{-\\frac{1}{2}}$ has eigenvalues in the range $[0, 2]$.\n",
    "\n",
    "3. Using the fact that $D^{-\\frac{1}{2}} A D^{-\\frac{1}{2}}$ is symmetric and thus has an eigendecomposition $D^{-\\frac{1}{2}} A D^{-\\frac{1}{2}} = U \\Lambda U^T$, prove that \n",
    "\n",
    "$$\n",
    "\\left( \\frac{1}{T} \\sum_{r=1}^{T}\\left(D^{-1} A\\right)^{r}\\right) D^{-1} = D^{-\\frac{1}{2}} U\\left( \\underbrace{\\frac{1}{T} \\sum_{r=1}^T \\Lambda^r}_{\\text{polynomial}} \\right) U^T D^{-\\frac{1}{2}} \\qquad \\qquad (2)\n",
    "$$\n",
    "\n",
    "4. We have noted that the inner sum is a polynomial on the eigenvalues. As we have realized, the eigenvalues range from $-1$ to $1$. Let's see how these polynomials behave. Plot the function $f(\\lambda) = \\frac{1}{T} \\sum_{r=1}^T \\lambda^r$ for different values of $T$. What do you observe, as $T$ increases?  \n",
    "    _Hint_: What happens to $\\lambda$s with negative and smaller values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.linspace(-1, 1, 20)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.set_xlabel(\"Eigenvalue $\\lambda_i$ (before filtering)\")\n",
    "ax.set_ylabel(\"Eigenvalue $\\lambda_i$ (after filtering)\")\n",
    "ax.set_title(\"$f(\\lambda_i) = \\\\frac{1}{T}\\sum_{r=1}^T \\lambda^r$\")\n",
    "for T in [1, 2, 5, 10]:\n",
    "    ### TODO Your code here\n",
    "    def f(l):\n",
    "        new_l = ...\n",
    "        return new_l / T\n",
    "    y = f(xs)\n",
    "    ### TODO Your code here\n",
    "    \n",
    "    ax.plot(xs, y, label=\"T: %d\" % T)\n",
    "ax.legend() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Keeping your observation from (4) in mind and say that we knew the eigendecomposition of $D^{-\\frac{1}{2}} A D^{-\\frac{1}{2}}$ mentioned above. How could we then change the algorithm for computing Equation from (1) to be faster at the cost of approximating $S$? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optionals\n",
    "## Exercise 6\n",
    "Let's try to do as in Exercise 3 - but with a standard Neural Network.\n",
    "That is: \n",
    "1. Define a two layer neural network, which takes as input size $n$ vectors and outputs two-dimensional predictions (with softmax).\n",
    "2. Train it on the two points with labels.\n",
    "3. Test it on the rest of the points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size=10):\n",
    "        super().__init__()\n",
    "        ### TODO code here\n",
    "        ### TODO code here\n",
    "    \n",
    "    def forward(self, X):\n",
    "        ### TODO code here\n",
    "        ### TODO code here\n",
    "        return F.log_softmax(X, 1)\n",
    "\n",
    "G = nx.karate_club_graph()\n",
    "simple_net = Net(G.order(), 2, 10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(simple_net.parameters(), lr=0.01)\n",
    "train_node_classifier(simple_net, optimizer, X, labels)\n",
    "\n",
    "y_pred = torch.argmax(simple_net(X), dim=1).numpy()\n",
    "y = labels.numpy()\n",
    "\n",
    "print(\"Confusion matrix:\")\n",
    "print(classification_report(y, y_pred, target_names=['I','A']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Why is the score here basically the same as random guessing?\n",
    "2. Why was this not the case in Exercise 3?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
