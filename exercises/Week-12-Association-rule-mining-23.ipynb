{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises Week 12 - Association Rule Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Local imports\n",
    "import sys\n",
    "sys.path.append('./utilities')\n",
    "from load_data import load_market_basket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.\n",
    "\n",
    "The work you do in this exercise will be very useful also in your hand-in.\n",
    "\n",
    "We learned the Apriori algorithm in class. Make your own implementation. \n",
    "\n",
    "We will use the anonymized real-world retail market basket data from: http://fimi.ua.ac.be/data/.\n",
    "\n",
    "This data comes from an anonymous Belgian retail store, and was donated by Tom Brijs from Limburgs Universitair Centrum, Belgium. The original data contains 16,470 different items and 88,162 transactions. You may only work with the top-50 items in terms of occurrence frequency.\n",
    "\n",
    "Your task is to:\n",
    "1. Implement the Apriori algorithm.\n",
    "2. Apply the Apriori algorithm on these data to find association rules with minimum support 0.05 and minimum confidence 0.7. Write down the rules in descending order of confidence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the retail data\n",
    "transactions = load_market_basket()\n",
    "\n",
    "def book_example():\n",
    "    return [\n",
    "        [1, 2, 4, 5],\n",
    "        [2, 3, 5],\n",
    "        [1, 2, 4, 5],\n",
    "        [1, 2, 3, 5],\n",
    "        [1, 2, 3, 4, 5], \n",
    "        [2, 3, 4],\n",
    "    ]\n",
    "\n",
    "def filter_transactions(T, k=50):\n",
    "    \"\"\"\n",
    "        Keep only the top k items in the transactions.\n",
    "        Remove transactions that become empty.\n",
    "    \"\"\"\n",
    "    # Count occurences of each item\n",
    "    counts = [0] * 16470\n",
    "    for t in T:\n",
    "        for i in t:\n",
    "            counts[i] += 1\n",
    "\n",
    "    # Sort and select top k\n",
    "    counts = np.array(counts)\n",
    "    order  = np.argsort(counts)[::-1] # reverse the sorted order\n",
    "\n",
    "    indexes_to_keep = order[:k]       # Keep the top k items\n",
    "    index_set = set(indexes_to_keep)  # Convert to python set for efficiency\n",
    "\n",
    "    # Filter transactions\n",
    "    T_new = [t_ for t_ in  [list(filter(lambda i: i in index_set, t)) for t in T]  if t_]\n",
    "    return T_new\n",
    "\n",
    "T = filter_transactions(transactions, k=50)\n",
    "#T = book_example() # You can first use the very small dataset to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations, chain\n",
    "# Tiny function for generating rules from tuples\n",
    "# Ex: rule((1, 2), (5)) outputs \"(1, 2) => (5)\"\n",
    "rule  = lambda lhs, rhs: \"%s => %s\" % (str(lhs), str(rhs)) # For generating rule strings\n",
    "\n",
    "\n",
    "def compute_support(Ck, T, still_applicable=None):\n",
    "    counts = None\n",
    "    ### TODO Your code here\n",
    "    # counts = np.zeros(len(Ck))\n",
    "    # print(counts)\n",
    "    # for idx, candidate in enumerate(Ck):\n",
    "    #     for transaction in T:\n",
    "    #         if(all(c in transaction for c in candidate)): counts[idx] += 1\n",
    "\n",
    "    #MAX:\n",
    "\n",
    "    #make c a tuple - list is an unhashable type\n",
    "    if still_applicable is None: still_applicable = [True] * len(T)\n",
    "    counts = {}\n",
    "    for row, t in enumerate(T):\n",
    "        if not still_applicable[row]: continue\n",
    "        # flag that records whether we found anything that is still a subset\n",
    "        found_any = False\n",
    "        for c in Ck:\n",
    "            if set.issubset(set(c), t):\n",
    "                if c in counts: counts[c] += 1\n",
    "                else: counts[c] = 1\n",
    "                found_any = True\n",
    "\n",
    "        still_applicable[row] = found_any\n",
    "\n",
    "    ### TODO Your code here\n",
    "    return counts\n",
    "\n",
    "def extend_prefix_tree(Ck_prev):\n",
    "    Ck = set()\n",
    "    ### TODO Your code here\n",
    "    # join step\n",
    "    for itemset in Ck_prev:\n",
    "        its1 = tuple(sorted(itemset))\n",
    "        for itemset2 in Ck_prev:\n",
    "            its2 = tuple(sorted(itemset2))\n",
    "            # if the k-1 first items are the same, we can join\n",
    "            if its1[:-1] == its2[:-1] and its1[-1] < its2[-1]: Ck.add(its1 + its2[-1:])   # FIXME: skal det være -1: eller -1\n",
    "    to_remove = set()\n",
    "    # if a subset of a rule does not exist in prev candidates, we should remove it - making it bigger will not make it more frequent/the rule more confident\n",
    "    for c in Ck:\n",
    "        for subset in combinations(c, len(c)-1):\n",
    "            if not subset in Ck_prev:\n",
    "                to_remove.add(c)\n",
    "                break\n",
    "    for c in to_remove:\n",
    "        Ck.remove(c)\n",
    "    ### TODO Your code here\n",
    "\n",
    "    return Ck\n",
    "\n",
    "def powerset(iterable):\n",
    "    ### TODO Your code here\n",
    "    s = list(iterable)\n",
    "    return chain.from_iterable(combinations(s, r) for r in range(1, len(s))) # any set that has at most length of the complete set or \n",
    "    ### TODO Your code here\n",
    "\n",
    "def apriori_algorithm(T, support=0.05, min_confidence=0.7):\n",
    "    \"\"\"\n",
    "        Apriori algorithm for mining association rules.\n",
    "        Inputs:\n",
    "            T:               A list of lists, each inner list will contiain integer-item-ids. \n",
    "                             Example: T = [[1, 2, 5], [2, 3, 4], [1, 6]]\n",
    "            support:         The proportion of occurences needed to keep itemsets.\n",
    "            min_confidence:  Minimum confidence for the algorithm to output the rule.\n",
    "        \n",
    "        Outputs:\n",
    "            rules:           List of tuples [(rule:str, confidence:float), ... ]\n",
    "                             Example: [(\"(1, 2) => (5)\", 0.84), (\"(3, 4) => (7)\", 0.75)]\n",
    "    \"\"\"\n",
    "    n = len(T)\n",
    "\n",
    "    itemsets = {}\n",
    "    C1 = set()\n",
    "    for t in T:\n",
    "        for ti in t: C1.add((ti,))\n",
    "    still_applicable = [True] * n\n",
    "    Ck = C1\n",
    "\n",
    "    k = 1\n",
    "    while Ck:\n",
    "        itemsets[k] = compute_support(Ck, T, still_applicable)\n",
    "        # if the itemset does not have the min support \n",
    "        Ck_copy = Ck.copy()\n",
    "        for itemset in Ck:\n",
    "            if itemsets[k][itemset] / n < support:\n",
    "                del itemsets[k][itemset]\n",
    "                Ck_copy.remove(itemset)\n",
    "        Ck = extend_prefix_tree(Ck_copy)\n",
    "        k += 1\n",
    "    # so far: only itemsets \n",
    "    \n",
    "    k = 2\n",
    "    rules = []\n",
    "    while k <= max(itemsets.keys()):\n",
    "        for itemset in itemsets[k].keys():\n",
    "            for rhs in powerset(itemset):\n",
    "                remaining = set(itemset).difference(set(rhs))\n",
    "                lhs = tuple(sorted(remaining))\n",
    "\n",
    "                conf = itemsets[k][itemset] / itemsets[len(lhs)][lhs]\n",
    "                if conf >= min_confidence:\n",
    "                    rules.append((rule(lhs, rhs), conf))\n",
    "        k += 1\n",
    "\n",
    "    ### TODO Your code here\n",
    "    \n",
    "    ### TODO Your code here\n",
    "    return rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you can try your algorithm. If you want, you could try changing the support and min_confidence to see what happens to the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conf.    \t Rule\n",
      " 0.8168% \t (41, 48) => (39,)\n",
      " 0.7681% \t (38, 48) => (39,)\n",
      " 0.7637% \t (41,) => (39,)\n"
     ]
    }
   ],
   "source": [
    "rules = apriori_algorithm(T, support=0.05, min_confidence=0.7)\n",
    "rules = sorted(rules, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"%-8s \\t %s\" % (\"Conf.\", \"Rule\"))\n",
    "for r in rules:\n",
    "    print(\"%7.4f%% \\t %s\" % r[::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.\n",
    "\n",
    "We have learned how to mine frequent itemsets and association rules from a transaction database where each transaction consists of a simple set of items. You are asked to propose a framework for mining association rules from transaction data, in which each item in a transaction is associated with an integer number that counts how many times the items appears in the transaction. In a market basket, this count number indicates the number of copies of a product in a customer’s basket. For example, we do not only care whether a customer bought fish or not, but how many pieces of fish they bought.\n",
    "\n",
    "1. Define (extend) the notion of an itemset and an association rule in the case of such data.\n",
    "2. Describe an efficient algorithm that mines itemsets and association rules as defined in Exercise 2. Illustrate the pruning strategies used in your algorithm and explain how they relate to the Apriori principle.\n",
    "3. Extend your implementation of the Apriori algorithm to handle this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO your code here\n",
    "# You can copy pase the code from above and adjust it\n",
    "### TODO your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "Consider a dataset of transactions $D$, and let $D'$ be a dataset derived from $D$ by independently deleting items from transactions in $D$. In particular, any item in any transaction in $D$ is deleted with probability $p$.\n",
    "\n",
    "1. Given an itemset $S$, compute its expected support in $D'$ as a function of its support in $D$.\n",
    "2. Compute the probability that an itemset $S$, which is frequent in $D$, is also frequent in $D'$, under the same minimum support threshold.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
