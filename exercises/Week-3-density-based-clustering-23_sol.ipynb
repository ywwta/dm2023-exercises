{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3 - Density Based Clustering\n",
    "\n",
    "During this weeks exercises, we will be working with density based clustering.\n",
    "In particular, we will be working with the algorithms DBSCAN, DENCLUE.\n",
    "\n",
    "First, lets import some stuff and plot the data that we are going to use today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Local imports\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "from utilities.load_data import load_iris_PC, load_t7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Theoretical questions\n",
    "- Please provide a brief description of what characterises density-based clustering as a clustering approach? How do both DBSCAN and DENCLUE define clusters, and what is the core difference between the two? \n",
    "\n",
    "**Solution**\n",
    "\n",
    "Desity-based clustering: clusters are density-connected points separated by sparse areas in feature space. \n",
    "\n",
    "Cluster: connectivity/maximality; \n",
    "Difference: \n",
    "DBSCAN: discrete kernel / counting within radius; core points.\n",
    "DENCLUE: Gaussian kernel; density attractors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: DBSCAN in Action\n",
    "Consider Figure 15.12 and answer the following questions, assuming that we use the\n",
    "Euclidean distance between points, and that $\\epsilon=2$ and $minpts = 3$\n",
    "1. List all the core points.\n",
    "1. Is $a$ directly density reachable from $d$?\n",
    "1. Is $o$ density reachable from $i$? Show the intermediate points on the chain or the point where the chain breaks.\n",
    "1. Is density reachable a symmetric relationship, that is, if $x$ is density reachable from $y$, does it imply that $y$ is density reachable from $x$? Why or why not?\n",
    "1. Is $l$ density connected to $x$? Show the intermediate points that make them density connected or violate the property, respectively.\n",
    "1. Is density connected a symmetric relationship?\n",
    "1. Show the density-based clusters and the noise points.\n",
    "1. If we use the manhattan distance instead, what is then the core points?\n",
    "\n",
    "![15.12](graphics/15.12.png)\n",
    "\n",
    "We have included the points in the code below, if you want to use `numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "    [5., 10., 11., 6., 10., 12., 13., 5., 10., 13., 6., 9., 11., 14., 15., 2., 3., 5., 6., 7., 15., 3., 7., 8.],\n",
    "    [8.,  8.,  8., 7.,  7.,  7.,  7., 6.,  6.,  6., 5., 4.,  5.,  6.,  5., 4., 4., 4., 4., 4.,  4., 3., 3., 2.]\n",
    "]).T\n",
    "labels = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $a$, $b$, $c$, $d$, $e$, $f$, $g$, $h$, $i$, $j$, $k$, $n$, $o$, $p$, $q$, $r$, $s$, $t$, $v$, $w$\n",
    "2. Yes. ($d$ is a core point and $a$ is in its neighborhood.)\n",
    "3. Yes, $i$->$e$->$f$->$j$->$n$->$o$.\n",
    "4. No, since a non-core object may be reachable from a core object, but the reverse is not necessarily true, i.e., in the chain x = $x_0$, $x_1$, ... , $x_l$, the points $x_1$, ... , $x_l$ need to be core points but not x0. For example $u$ is density reachable from $n$ but $n$ is not density reachable from $u$.\n",
    "5. Yes, $t$ is the intermediate point.\n",
    "6. Yes, by definition.\n",
    "7. Clusters see code and figure below. No noise points.\n",
    "8. Here core points are exact the same. However, the situation would change if we change epsilon or dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "db = DBSCAN(eps=2., min_samples=3).fit(X)\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.scatter(*(X.T), c=db.labels_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Implement DBSCAN\n",
    "\n",
    "In this exercise, we will try to implement the DBSCAN algorithm. \n",
    "You are allowed to structure your code however you want. \n",
    "The code below is inspired by [Zaki, p. 377] and serves as inspiration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L2_norm(x, y):\n",
    "    return np.sqrt(np.sum((x-y)**2))\n",
    "\n",
    "def densityConnect(x,core,cl,Nx,k): # You might need more parameters here.\n",
    "    for i in Nx[x]:\n",
    "        if cl[i]==-1:\n",
    "            cl[i]=k\n",
    "            if i in core:\n",
    "                cl=densityConnect(i,core,cl,Nx,k)\n",
    "    return cl\n",
    "\n",
    "\n",
    "def dbscan(X, e, m, dist_fn=L2_norm):\n",
    "    \"\"\"\n",
    "        Args:\n",
    "            X:       Matrix of shape [n, d] with data points on the rows.\n",
    "            e:       Epsilon distance for neighborhood calculations.\n",
    "            m:       Minimum neighbors in epsilon neighborhood for a point to be a core point.\n",
    "            dist_fn: Distance function to be used.\n",
    "        \n",
    "        Returns:\n",
    "            clusters:   A vector of shape [n,] with integers, indicating cluster assignments.\n",
    "                        Let clusters[i] == -1 if point x_i is an outlier and a non-negative \n",
    "                        integer corresponding to the cluster index of point x_i otherwise.\n",
    "    \"\"\"\n",
    "    \n",
    "    n=X.shape[0]\n",
    "    core = []\n",
    "    Nx = []\n",
    "    for i in range(n):\n",
    "        nx = []\n",
    "        for j in range(n):\n",
    "            if L2_norm(X[i],X[j]) <= e:\n",
    "                nx.append(j)\n",
    "        if len(nx) >= m:\n",
    "            core.append(i)\n",
    "        Nx.append(nx)\n",
    "\n",
    "    clusters = np.array([-1]*n,dtype=int)\n",
    "    k = 0\n",
    "    for c in core:\n",
    "        if clusters[c]==-1:\n",
    "            clusters=densityConnect(c,core,clusters,Nx,k)\n",
    "            k+=1\n",
    "    \n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is some code to try out your implementation.\n",
    "You can, of course, fiddle with the parameters and see what happens to the clusters.\n",
    "The parameters provided below whould work relatively well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test clustering on iris_PC dataset\n",
    "# clustering = dbscan(X, 0.5, 5) # Works for iris_2PC dataset\n",
    "slow = True\n",
    "if slow:\n",
    "    X, _ = load_t7()\n",
    "    clustering = dbscan(X, e=15, m=10)\n",
    "else: \n",
    "    X, _ = load_iris_PC()\n",
    "    clustering = dbscan(X, e=0.4, m=5)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.scatter(*(X.T), c=clustering)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: DENCLUE Calculations\n",
    "This exercise relates to the DENCLUE algorithm introduced in Section 15.3 in [Zaki].\n",
    "Consider the points shown in Figure 15.13. For the Gaussian kernel\n",
    "\n",
    "$$\n",
    "K(\\mathbf{z})=\\frac{1}{(2 \\pi)^{d / 2}} \\exp \\left\\{-\\frac{\\mathbf{z}^{T} \\mathbf{z}}{2}\\right\\},\n",
    "$$\n",
    "\n",
    "![](graphics/15.13.png)\n",
    "\n",
    "answer the following questions assuming that $h = 2$:\n",
    "\n",
    "1. What is the probability density at e?\n",
    "2. What is the gradient at e? (Try to actually derive the gradient of $\\hat f(x)$ your self)\n",
    "3. List all the density attractors for this dataset.\n",
    "\n",
    "As before, if you want to use `numpy`, we have included the points below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**density estimation**\n",
    "\n",
    "For d-dimensional data $x = (x1, ..., xd)$, the window $h$ becomes a hypercube centered at $x$ with volume $h$, that is $vol(Hd(h)) = hd$. Density estimation is the same as above, except that the density is now divided by the volume of the hypercube. Here $h=2$\n",
    "\n",
    "$$\\hat{f(e)} = \\frac{1}{nh^d}\\sum K(\\frac{e-x}{h})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "    [5., 6., 6., 2., 3., 5., 7., 9., 3., 8., 7.],\n",
    "    [8., 7., 5., 4., 4., 4., 4., 4., 3., 2., 5.]\n",
    "]).T\n",
    "labels = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k']\n",
    "\n",
    "#a)\n",
    "n, d = X.shape\n",
    "\n",
    "K = lambda z: np.exp(-np.dot(z, z)/2) / (2*np.pi)**(d/2)\n",
    "h = 2.\n",
    "e = X[4]\n",
    "\n",
    "f_hat = np.sum([K((e-X[i])/h) for i in range(n)])  / (n*h**d)\n",
    "print(\"f(e) = %.4f\" % f_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to discover \"peaks\", in density funtion, we compute its gradient.\n",
    "\n",
    "$$\\bigtriangledown \\hat{f(e)} = \\frac{1}{nh^{d+2}}\\sum K(\\frac{e-x}{h})\\cdot(e-x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad = lambda p: -np.sum([K((p-x)/h)*(p-x) for x in X], axis=0) / (n*h**(d+2))\n",
    "print(\"f'(e) = %s\" % grad(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**repeat**\n",
    "$$X_{t+1}\\gets \\frac{\\sum_{i}^nK(\\frac{x-x_i}{h})x_i}{\\sum_{i}^nK(\\frac{x-x_i}{h})}$$\n",
    "**until** $||x_t-x_{t-1}||< \\epsilon$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_ascent(p):\n",
    "    old = p.copy()\n",
    "    new = p.copy()\n",
    "    first = True\n",
    "    olds = []\n",
    "    while first or np.sqrt(((old-new)**2).sum()) > 1e-6:\n",
    "        olds.append(old)\n",
    "        old = new\n",
    "        first = False\n",
    "        \n",
    "        new = old + 0.3 * grad(old)\n",
    "        \n",
    "    return new, np.array(olds)\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "ax.scatter(*(X.T))\n",
    "for x in X: \n",
    "    attractor, path = gradient_ascent(x)\n",
    "    print(attractor, x)\n",
    "    ax.plot(*(path.T))\n",
    "plt.savefig('density-attractor.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Contingency table \n",
    "\n",
    "Provide an implementation of the contingency table and plot the table varying the DBScan parameters. Use iris_pc dataset. \n",
    "1. Can you use the contingengy table to evaluate the quality of clustering? \n",
    "2. Can you compute purity based on the contingency table?\n",
    "3. Can you find the best parameter setting using purity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,ground_truth = load_iris_PC()\n",
    "n=ground_truth.shape[0]\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "def convert(Y): # convert nxc one-hot matrix to nx1 vector\n",
    "    res=np.ndarray(Y.shape[0])\n",
    "    for i in range(Y.shape[0]):\n",
    "        res[i]=np.where(Y[i]==1)[0]\n",
    "    return res\n",
    "\n",
    "ground_truth=convert(ground_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Contingency table**\n",
    "\n",
    "$$N_{ij} = n_{ij} = |C_i\\cap T_j|$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contingency_table(C, T): \n",
    "    \"\"\"\n",
    "    Args:\n",
    "        C:       Clusters obtained by a clustering algorithm as a nx1 vector\n",
    "        T:       Ground-truth cluster assignments as a nx1 vector\n",
    "\n",
    "    Returns:\n",
    "        ctable:   a num_clusters_of_C x num_cluster_of_T matrix containing the overlaps among the different clusters\n",
    "    \"\"\"\n",
    "    \n",
    "    ctable = np.zeros([int(np.max(C))+1,int(np.max(T))+1],dtype=int)\n",
    "    n = len(C)\n",
    "    for i in range(n):\n",
    "        if (C[i]>=0):\n",
    "            ctable[int(C[i]),int(T[i])]+=1\n",
    "            \n",
    "    return ctable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**purity**\n",
    "\n",
    "$$purity_i = \\frac{1}{|C_i|}max_{j=1..k}\\{n_{ij}\\}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def purity(ctable,n):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        ctable:   a num_clusters_of_C x num_cluster_of_T matrix containing the overlaps among the different clusters\n",
    "\n",
    "    Returns:\n",
    "        purity:   a float, the purity of clustering result\n",
    "    \"\"\"\n",
    "    purity = 0.0\n",
    "    w=ctable.max(1)\n",
    "    purity=np.sum(w)/n\n",
    "    return purity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_lst = np.linspace(0.1, 1, 19) #[0.1,0.15,...,1] for eps\n",
    "minpts_lst = np.linspace(2, 15, 14) #[2,3,...,15] for minpts\n",
    "purity_table = np.zeros((len(eps_lst), len(minpts_lst)))\n",
    "\n",
    "for i in range(len(eps_lst)):\n",
    "    for j in range(len(minpts_lst)):\n",
    "        cl = dbscan(X,eps_lst[i], minpts_lst[j]) #use your dbscan\n",
    "        ctable = contingency_table(cl, ground_truth) #use your contingency table function above\n",
    "        prt = purity(ctable, n) #use your purity function above\n",
    "        purity_table[i, j] = prt\n",
    "print(purity_table) #purity with different parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(purity_table)\n",
    "display(df)\n",
    "best = np.unravel_index(np.argmax(purity_table, axis = None), purity_table.shape)\n",
    "print(f\"Best parameter setting is: {best},\\nThe maximal purity is: {purity_table[best]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6: DBSCAN with Gaussian kernel\n",
    "\n",
    "In this exercise, we are going to alter the DBSCAN algorithm to use another kernel.\n",
    "\n",
    "As stated in [Zaki, p.388], DBSCAN is a special case of DENCLUE.\n",
    "In particular, if we let $h=\\epsilon$ and $\\xi = minPts$ with a discrete kernel, then the two algorithms will yield the same result.\n",
    "\n",
    "We are now going to go a step in the other direction.\n",
    "Thas is, we will add a Gaussian kernel to the DBSCAN algorithm.\n",
    "The Gaussian kernel is defined as in Equation (1).\n",
    "\n",
    "$$\n",
    "K(\\mathbf{z})=\\frac{1}{(2 \\pi)^{d / 2}} \\exp \\left\\{-\\frac{\\mathbf{z}^{T} \\mathbf{z}}{2}\\right\\}\n",
    "\\qquad \\qquad (1)\n",
    "$$\n",
    "\n",
    "The implications in terms of the algorithm are the following:\n",
    "\n",
    "1. When selecting core points, they are now going to depend on a threshold $\\xi$ of the density estimates $\\hat f(x)$.\n",
    "2. The threshold $\\epsilon$ is now going to be compared against $K(\\frac{x-x_i}{h})$ when calculating neighborhoods and density connectedness.\n",
    "3. The value $h$ which was previously fixed to $\\epsilon$ is now going to be a parameter to the model.\n",
    "\n",
    "As before, the code below serves as inspiration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def K(z):\n",
    "    return  np.exp(-np.dot(z, z)/2) / np.sqrt(2*np.pi)\n",
    "\n",
    "def densityConnect(x,core,cl,Nx,k): # You might need more parameters here.\n",
    "    for i in Nx[x]:\n",
    "        if cl[i]==-1:\n",
    "            cl[i]=k\n",
    "            if i in core:\n",
    "                cl=densityConnect(i,core,cl,Nx,k)\n",
    "    return cl\n",
    "\n",
    "def gaussian_dbscan(X, e, m, h):\n",
    "    \"\"\"\n",
    "        Args:\n",
    "            X:       Matrix of shape [n, d] with data points on the rows.\n",
    "            e:       Epsilon distance for neighborhood calculations.\n",
    "            m:       Minimum neighbors in epsilon neighborhood for a point to be a core point.\n",
    "            dist_fn: Distance function to be used.\n",
    "        \n",
    "        Returns:\n",
    "            clusters:   A vector of shape [n,] with integers, indicating cluster assignments.\n",
    "                        Let clusters[i] == -1 if point x_i is an outlier and a non-negative \n",
    "                        integer corresponding to the cluster index of point x_i otherwise.\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO, code here.\n",
    "    n=X.shape[0]\n",
    "    core = []\n",
    "    Nx = []\n",
    "    for i in range(n):\n",
    "        d = 0\n",
    "        nx = []\n",
    "        for j in range(n):\n",
    "            dj=K((X[i]-X[j])/h)/n*h**d\n",
    "            if dj >= e and i != j:\n",
    "                nx.append(j)\n",
    "            d+=dj\n",
    "        Nx.append(nx)\n",
    "        if d>=m:\n",
    "            core.append(i)\n",
    "    clusters = np.array([-1]*n,dtype=int)\n",
    "    k=0\n",
    "    for c in core:\n",
    "        if clusters[c]==-1:\n",
    "            clusters=densityConnect(c,core,clusters,Nx,k)\n",
    "            k+=1\n",
    "    \n",
    "    return clusters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the code below to run your algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test clustering on iris_PC dataset\n",
    "np.random.seed(0)\n",
    "\n",
    "slow = True\n",
    "if slow:\n",
    "    X, _ = load_t7()\n",
    "    n, _ = X.shape\n",
    "    X = X[np.random.permutation(n)]\n",
    "    X = X[:2000]\n",
    "    clustering = gaussian_dbscan(X, e=0.00000006, m=0.00018, h=3.25)\n",
    "else: \n",
    "    X, _ = load_iris_PC()\n",
    "    clustering = gaussian_dbscan(X, e=0.2, m=12., h=0.7)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "sc = ax.scatter(*(X[clustering >= 0].T), c=clustering[clustering>=0])\n",
    "sc = ax.scatter(*(X[clustering < 0].T), marker='x')\n",
    "plt.colorbar(sc)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7: Comparing the DBSCAN versions above\n",
    "Please describe how the two versions of DBSCAN above differ.\n",
    "How do you think the the differences affects the clustering of the data?\n",
    "\n",
    "Did you see any practical differences in the \"experiments\" above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
