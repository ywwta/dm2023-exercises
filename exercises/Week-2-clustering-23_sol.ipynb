{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 (K-means, K-medoids, Gaussian Mixtures)\n",
    "\n",
    "This week we are going to work with K-means, K-medoids, and Gaussian Mixtures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Local imports (used for the last optional exercise.)\n",
    "import math\n",
    "import itertools\n",
    "import sys\n",
    "sys.path.append(\"../utilities\")\n",
    "#from gmm import GMM\n",
    "#from load_data import load_iris, load_iris_PC, index_to_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Warmup\n",
    "Please provide a brief description of what characterises \n",
    "1. Clustering as a task \n",
    "2. Representative-based clustering as a clustering approach "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "1. Unsupervisely group the data such that: Highly similar to any other point in the cluster; Dissimilar to any other point in another cluster.\n",
    "2. Use representative per cluster (mean/medoid), iterative approach to improve representation (mean/medoid plus covariances) and assignment (cluster memberships and probabilities)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Practical K-means\n",
    "Given the following points: 2, 4, 10, 12, 3, 20, 30, 11, 25. Assume $k=3$, and that we randomly pick the initial means $\\mu_1=2$, $ \\mu_2=4$ and $\\mu_3=6$. Show the clusters obtained using K-means algorithm after one iteration, and show the new means for the next iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "\n",
    "X = np.array([2., 4., 10., 12., 3., 20., 30., 11., 25.])\n",
    "mus = np.array([[2., 4., 6.]])\n",
    "\n",
    "# Setup\n",
    "iterations = 4\n",
    "colormap = cm.get_cmap('plasma')\n",
    "colors = colormap([0., 0.3, 0.6])\n",
    "fig, ax = plt.subplots(iterations,2, figsize=(12,iterations*2))\n",
    "\n",
    "def plot(idx, mus, clusters): \n",
    "    \"\"\"\n",
    "        Function for plotting each iteration.\n",
    "    \"\"\"\n",
    "    if idx % 2 == 0: ax[idx//2, 0].set_title(\"Iteration %i, clusters\" % (idx//2))  # Set titles\n",
    "    else:            ax[idx//2, 1].set_title(\"Iteration %i, new means\" % (idx//2))\n",
    "    ax[idx//2, idx%2].get_yaxis().set_visible(False) # Hide y-axis\n",
    "    \n",
    "    # Plot ponits and means from each cluster.\n",
    "    for j, mu in enumerate(mus):\n",
    "        ax[idx//2, idx%2].eventplot(X[clusters==j], lineoffsets=0, linelength=1, colors=[colors[j]])\n",
    "        ax[idx//2, idx%2].eventplot([mu], lineoffset=0, linelength=0.3, linewidth=5, colors=colors[j])\n",
    "\n",
    "X_ = X.reshape((X.shape[0], 1)) # Reshape X to have shape [n, 1]\n",
    "for i in range(iterations):\n",
    "    dists = np.sqrt(((X_-mus)**2))\n",
    "    clusters = np.argmin(dists, axis=1)\n",
    "    \n",
    "    plot(i*2, mus[0], clusters)\n",
    "    for j in range(mus.shape[1]):\n",
    "        mus[0,j] = X[clusters==j].mean()\n",
    "    plot(i*2+1, mus[0], clusters)\n",
    "    print(i, mus)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "Which algorithm is more robust: k-means or k-medoid and why? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "Revisit robustness of mean/medoid from last week; as a consequence, the shapes of the clusters are more impacted by outliers when using k-means. (draw a small example where you add an extreme outlier and have them find mean/medoid and what would happen in the algorithm.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Practical Mixture of Gaussians\n",
    "Given the data points in Table 13.1, and their probability of belonging to two clusters.\n",
    "Assume that these points were produced by a mixture of two univariate normal distributions. \n",
    "Answer the following questions:\n",
    "\n",
    "1. Find the maximum likelihood estimate of the means $\\mu_1$ and $\\mu_2$\n",
    "2. Assume that $\\mu_1 = 2$, $\\mu_2 = 7$, and $\\sigma_1 = \\sigma_2 = 1$. Fint the probability that the point $x=5$ belongs to cluster $C_1$ and to cluster $C_2$. You may assume that the prior probability of each cluster is equal (i.e., $P(C_1) = P(C_2) = 0.5$), and the prior probability $P(x=5) = 0.029$\n",
    "\n",
    "\n",
    "![Table 13.1](graphics/13.1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want, you can use python here.\n",
    "X            = np.array([2, 3, 7, 9, 2, 1])\n",
    "P_C1_given_x = np.array([0.9, 0.8, 0.3, 0.1, 0.9, 0.8])\n",
    "P_C2_given_x = 1 - P_C1_given_x\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML estimate of means\n",
    "# Part 1\n",
    "P = np.c_[P_C1_given_x, P_C2_given_x] # Probs of shape  [n, 2]\n",
    "X_ = np.expand_dims(X, 1)             # Make X shape    [n, 1]\n",
    "\n",
    "mus = (X_ * P).sum(0) / P.sum(axis=0)\n",
    "print(\"Maximum Likelihood estimate of means:\", mus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2\n",
    "mus = np.array([2., 7.])\n",
    "std = np.array([1., 1.])\n",
    "\n",
    "# Evaluate normal distributions to get p(x|y)\n",
    "probs = np.array([norm.pdf(5., loc=mu, scale=st) for mu, st in zip(mus, std)])\n",
    "# P(y | x) = p(x|y) * p(y) / p(x)\n",
    "# P(y | x) = N(mu, sigma) * 0.5 / 0.029\n",
    "p_xy = probs * 0.5 / 0.029\n",
    "\n",
    "print(\"Posterior probabilities: \", p_xy)\n",
    "assert np.allclose(1, np.sum(p_xy), atol=0.01), \"Should sum to 1.\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "For which parameter $\\mu,\\Sigma,P(C)$ settings is EM clustering identical to k-means clustering and why?<br>\n",
    "Mean $\\mu_i = \\frac{{\\sum_{j=1}^{n}{x_j} w_{ij}}}{\\sum_{j=1}^{n}{w_{ij}}}$<br>\n",
    "Covariance $\\Sigma_i = \\frac{\\sum_{j=1}^{n}w_{ij}(x_j - \\mu_i)(x_j - \\mu_i)^\\top}{\\sum_{j=1}^{n} w_{ij}}$<br>\n",
    "Prior $P(c_i) = \\frac{\\sum_{j=1}^{n}w_{ij}}{n}$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "Note that the formulas are not too far from k-means. In fact, since in k-means a point is either assigned or not to a single cluster, we can assume that $w_{ij} = 1$ if $x_j \\in C_i$ and $w_{ij} = 0$ if $x_j \\in C_i$. In such a case, the formula for the mean becomes exactly the centroid, while the covariance becomes the radius, and the prior the probability of drawing randomly a point from cluster $C_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6: 2d K-means and gaussian mixture\n",
    "Given the two-dimensional points in Table 13.2, assume that $k=2$, and that initially the points are assigned to clusters as follors: $C_1 = \\{ x_1, x_2, x_4 \\}$ and $C_2 = \\{ x_3, x_5 \\}$.\n",
    "Answer the following questions:\n",
    "\n",
    "1. Apply the K-means algorithm until convergence, that is, the clusters do not change, assuming (1) the usual Euclidean distance of the $L_2$-norm as the distance between points, defined as\n",
    "\n",
    "$$\n",
    "||x_i - x_j||_2 = \\sqrt{ \\sum_{a=1}^d (x_{ia} - x_{ja})^2 }\n",
    "$$\n",
    " and (2) the Manhattan distance of the $L_1$-norm\n",
    "$$\n",
    "||x_i - x_j||_1 = \\sum_{a=1}^d |x_{ia} - x_{ja}|.\n",
    "$$\n",
    "\n",
    "2. Apply the EM algorithm with $k=2$ assuming that the dimensions are independent. Show one complete execution of the expectation and the maximization steps. Start with the assumption that $P(C_i | x_{ja}) = 0.5$ for $a=1, 2$ and $j=1, ..., 5$.\n",
    "\n",
    "![Table 13.2](graphics/13.2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, if you want, you can use a bit of Python\n",
    "X = np.array([\n",
    "    [0, 0, 1.5, 5, 5],\n",
    "    [2, 0,   0, 0, 2]\n",
    "]).T # shape [5, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-means\n",
    "colormap = cm.get_cmap('plasma')\n",
    "colors = colormap([0., 0.3, 0.6])\n",
    "iterations = 3\n",
    "\n",
    "def plot(ax, i, j, X, mu, clusters):\n",
    "    K = np.max(clusters)+1\n",
    "    for k in range(K):\n",
    "        ax[i,j].scatter(*(X[clusters==k].T), c=[colors[k]])\n",
    "        ax[i,j].scatter([mu[k,0]], [mu[k,1]], c=[colors[k]], marker='v')\n",
    "\n",
    "def kmeans(dist_fn, minimize_fn, axes, iterations=3, k=2):\n",
    "    clusters = np.array([0, 0, 1, 0, 1])\n",
    "    for i in range(iterations):\n",
    "        mu       = np.array([minimize_fn(X[clusters==i], axis=0) for i in range(k)])\n",
    "        plot(axes, i, 0, X, mu, clusters)\n",
    "        \n",
    "        dists = np.array([[dist_fn(mui, x) for mui in mu] for x in X])\n",
    "        clusters = np.argmin(dists, axis=1)\n",
    "        plot(axes, i, 1, X, mu, clusters)\n",
    "\n",
    "L2_norm = lambda x, y: np.sqrt(((x - y)**2).sum())\n",
    "L1_norm = lambda x, y: np.sum(np.abs(x-y))\n",
    "\n",
    "fig, ax  = plt.subplots(iterations, 4, figsize=(9,7))\n",
    "ax[0,0].set_title(\"L2\")\n",
    "ax[0,2].set_title(\"L1\")\n",
    "\n",
    "kmeans(L2_norm, np.mean, ax[:,:2])\n",
    "kmeans(L1_norm, np.mean, ax[:,2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mu_i = \\frac{\\sum_{j=1}^N w_{ij}x_i}{\\sum_{j=1}^N w_{ij}}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian mixtures\n",
    "n, d  = X.shape\n",
    "k     = 2\n",
    "\n",
    "P     = np.ones((n, k, 1)) * 0.5             # [n, k, 1]\n",
    "Pk    = P.sum(axis=0)                        # [k,1]\n",
    "\n",
    "X_    = X.reshape((n, 1, d))                 # [n, 1, d]\n",
    "XP    = X_ * P                               # [n, k, d]\n",
    "mu    = XP.sum(0) / Pk                       # [k, d]\n",
    "\n",
    "print(\"Means: \\n\", mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\sum C_i = \\frac{\\sum_{j=1}^N w_{ij} (x_j-\\mu_i)(x_j-\\mu_i)^T}{\\sum_{j=1}^N w_{ij}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu    = mu.reshape((1, k, d))                # [1, k, d]\n",
    "X_    = X_ - mu                              # [n, k, d]\n",
    "X__   = (X_ * P).reshape(n, k, d, 1)         # [n, k, d, 1]\n",
    "X_    = X_.reshape(n, k, 1, d)               # [n, k, 1, d]\n",
    "outer = (X__ @ X_) * np.eye(d)               # [n, k, d, d] Independence assumption allows us to ignore off diagonal\n",
    "\n",
    "Cov   = outer.sum(0) / Pk.reshape((k,1,1))   # [k, d, d]\n",
    "mu = mu.reshape(k,d)\n",
    "print()\n",
    "print(\"Covariance: \\n\", Cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(C_i) = \\frac{\\sum_{j=1}^n w_{ij}}{n}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pri = (P.sum(0)/n).reshape(-1)\n",
    "print()\n",
    "print(\"Prior:\\n\",Pri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(C_i|\\mathbf{x})=\\frac{P(\\mathbf{x}|C_i)P(C_i)}{\\sum_{i=1}^{k}P(\\mathbf{x}|C_i)P(C_i)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Posterior:\")\n",
    "P = np.zeros((5, 2))\n",
    "for i in range(2):\n",
    "    P[:,i] = GMM.prob(X, mu[i], Cov[i])\n",
    "            \n",
    "print(P / P.sum(axis=1, keepdims=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optionals\n",
    "## Exercise 7\n",
    "Consider 2D data (2,2), (2,1), (2,3), (1,2), (3,2), (8,2), (8,1), (8,0), (8,3), (8,4), (7,2), (6,2), (9,2), (10,2), (7,1), (7,3), (9,1), (9,3)  \n",
    "\n",
    "![Data plotted](graphics/two_cluster_dataplot.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = np.array([\n",
    "    (2,2), (2,1), (2,3), (1,2), (3,2), (8,2), (8,1), (8,0), (8,3), (8,4), (7,2), (6,2), (9,2), (10,2), (7,1), (7,3), (9,1), (9,3) \n",
    "])\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "ax  = fig.add_subplot(121)\n",
    "ax.scatter(*(points.T))\n",
    "\n",
    "from sklearn.cluster import k_means\n",
    "_, labels, *_ = k_means(points, n_clusters=2)\n",
    "ax = fig.add_subplot(122)\n",
    "ax.scatter(*(points.T), c=labels)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let k=2 and sketch visually what you think the final clustering will be and explain why. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 8: K-means and the Iris dataset\n",
    "\n",
    "In this exercise, we will apply K-means to the two 2PC dataset from [Zaki] (and slides from Week 2).\n",
    "You may use the code below as inspiration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_iris_PC()\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "ax[0].scatter(*(X.T), c=y)\n",
    "ax[0].set_title(\"Real clusters\")\n",
    "\n",
    "\n",
    "def kmeans(X,k):\n",
    "    \"\"\"\n",
    "        Arguments:\n",
    "            k: int specifying number of clusters\n",
    "        Returns:\n",
    "            clusters: Array of indicators (ints) indicating the cluster of each point. Shape: [n,]\n",
    "    \"\"\"\n",
    "    n, d = X.shape\n",
    "    clusters = np.zeros((n,))\n",
    "    new_clusters=np.ones((n,))\n",
    "    centroids = np.random.randn(k, d) # K clusters of shape d\n",
    "    \n",
    "    while clusters is None or (not np.allclose(clusters, new_clusters)):\n",
    "        clusters = new_clusters\n",
    "        new_clusters = np.argmin([[L2_norm(X[i],centroids[j])for j in range(k)]for i in range(n)],axis=1) # assign points to clusters\n",
    "        centroids = [np.mean(X[np.where(new_clusters==i)],axis=0)for i in range(k)] # reassign centroids\n",
    "    return clusters\n",
    "\n",
    "clusters = kmeans(X,k=3)\n",
    "ax[1].scatter(*(X.T), c=clusters)\n",
    "ax[1].set_title(\"K-means clusters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 9: Gaussian Mixtures and the EM-Algorithm\n",
    "In this exercise, we will implement the EM-algorithm for the Gaussian Mixture Model.\n",
    "You can consult [Zaki] Section 13.3.2, for a description of how the algorithm works in this particular setup.\n",
    "\n",
    "Below is an extension of a Gaussian Mixture Model stub (`GMM`) found [here](../utilities/gmm.py), which has the method signatures for the unimplemented functions. Try to fill out the methods and run the experiment below afterwards.\n",
    "\n",
    "Besides the methods shown here, the `GMM` class has both a `fit` and a `predict` method, which both takes as input the data and returns `void` and cluster indexes, respectively. Both will use the functions that you implement below. Additionally, the number of gaussian mixtures `K` can be accessesed by `self.K`.\n",
    "\n",
    "Finally, the `GMM` class has a static function `prob`, which returns the values of a Gaussian pdf, given data, mean, and covariance matrix; use it if you please."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGMM(GMM):\n",
    "    def initialize_parameters(self, X):\n",
    "        \"\"\"\n",
    "            This function should utilize information from the data to initialize\n",
    "            the parameters of the model.\n",
    "            In particular, it should compute initial values for mu, Sigma, and pi.\n",
    "            \n",
    "            The function corresponds to line 2-4 in Algorithm 13.3 in [Zaki, p. 349]\n",
    "            Note, that K can be retrieved as `self.K`.\n",
    "\n",
    "            Args:\n",
    "                X (matrix, [n, d]): Data to be used for initialization.\n",
    "\n",
    "            Returns:\n",
    "                Tuple (mu, Sigma, pi), \n",
    "                    mu has size        [K, d]\n",
    "                    Sigma has size     [K, d, d]\n",
    "                    pi has size        [K]\n",
    "        \"\"\"\n",
    "        # TODO: what should the values be for initializing mu, Sigma and pi\n",
    "        return mu, Sigma, pi\n",
    "\n",
    "\n",
    "    def posterior(self, X):\n",
    "        \"\"\"\n",
    "            The E-step of the EM algorithm. \n",
    "            Returns the posterior probability p(Y|X)\n",
    "\n",
    "            This function corresponds to line 8 in Algorithm 13.3 in [Zaki, p. 349]\n",
    "            Note, that mean and covariance matrices can be accessed by `self.mu` and `self.Sigma`, respectively.\n",
    "            \n",
    "            Args:\n",
    "                X (matrix, [n,  d]): Data to compute posterior for.\n",
    "\n",
    "            Returns:\n",
    "                Matrix of size        [n, K]\n",
    "        \"\"\"\n",
    "        # TODO: what is the posterior probability?\n",
    "        \n",
    "        return posterior\n",
    "        \n",
    "\n",
    "    def m_step(self, X, P):\n",
    "        \"\"\"\n",
    "            Update the estimates of mu, Sigma, and pi, given the data `X` and the current\n",
    "            posterior probabilities `P`.\n",
    "\n",
    "            This function corresponds to line 10-12 in Algorithm 13.3 and Eqn. (13.11-13) in [Zaki, p. 349].\n",
    "            \n",
    "            Args:\n",
    "                X (matrix, [n, d]): Data matrix\n",
    "                P (matrix, [n, K]): The posterior probabilities for the n samples.\n",
    "\n",
    "            Returns:\n",
    "                Tuple (mu, Sigma, pi), \n",
    "                    mu has size        [K, d]\n",
    "                    Sigma has size    [K, d, d]\n",
    "                    pi has size        [K]\n",
    "        \"\"\"\n",
    "        # TODO: what is the values of mu, Sigma, and pi that maximizes the expectation given the posterior?\n",
    "        return  mu_hat, Si_hat, pi_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGMM(GMM):\n",
    "\tdef initialize_parameters(self, X):\n",
    "\t\t\"\"\"\n",
    "\t\t\tThis function should utilize information from the data to initialize\n",
    "\t\t\tthe parameters of the model.\n",
    "\t\t\tIn particular, it should update self.mu and self.Sigma.\n",
    "\n",
    "\t\t\tArgs:\n",
    "    \t\t\tX (matrix, [n, d]): Data to be used for initialization.\n",
    "\n",
    "\t\t\tReturns:\n",
    "    \t\t\tTuple (mu, Sigma, pi), \n",
    "\t\t\t\t\tmu has size\t\t[K, d]\n",
    "\t\t\t\t\tSigma has size\t[K, d, d]\n",
    "\t\t\t\t\tpi has size\t\t[K]\n",
    "\t\t\"\"\"\n",
    "\t\tn, d\t= X.shape\n",
    "\t\tK\t\t= self.K\n",
    "\n",
    "\t\tX_\t\t= X[np.random.permutation(n)] # Shuffle data\n",
    "\n",
    "\t\tmu\t\t= np.zeros((self.K, d))\n",
    "\t\tsize\t= n // K\n",
    "\n",
    "\t\tfor i in range(K):\n",
    "\t\t\tmu[i] = np.mean(X_[i*size:(i+1)*size])\n",
    "\n",
    "\t\tSigma\t= np.tile(np.expand_dims(np.eye(d), 0), (K, 1, 1)) * 0.1 # (K, d, d)\n",
    "\t\t\n",
    "\t\tpi\t\t= np.ones((self.K)) / self.K\n",
    "\t\treturn mu, Sigma, pi\n",
    "\n",
    "\n",
    "\tdef prior(self): \n",
    "\t\t\"\"\"\n",
    "\t\t\tReturns the prior probabilities p(Y).\n",
    "\n",
    "\t\t\tReturns:\n",
    "\t\t\t\tVector of size\t\t[K]\n",
    "\t\t\"\"\"\n",
    "\t\treturn self.pi\n",
    "\n",
    "\n",
    "\tdef posterior(self, X):\n",
    "\t\t\"\"\"\n",
    "\t\t\tThe E-step of the EM algorithm. \n",
    "\t\t\tReturns the posterior probability p(y|X)\n",
    "\n",
    "\t\t\tArgs:\n",
    "\t\t\t\tX (matrix, [n,  d]): Data to compute posterior for.\n",
    "\n",
    "\t\t\tReturns:\n",
    "\t\t\t\tMatrix of size\t\t[n, K]\n",
    "\t\t\"\"\"\n",
    "\t\tP = np.zeros((X.shape[0], self.K))\n",
    "\t\tfor i in range(self.K):\n",
    "\t\t\tP[:,i] = GMM.prob(X, self.mu[i], self.Sigma[i])*self.pi[i]\n",
    "\n",
    "\t\treturn P / P.sum(axis=1, keepdims=True) # Normalize\n",
    "\n",
    "\n",
    "\tdef m_step(self, X, P):\n",
    "\t\t\"\"\"\n",
    "\t\t\tUpdate the estimates of mu, Sigma, and pi, given the current\n",
    "\t\t\tposterior probabilities.\n",
    "\n",
    "\t\t\tArgs:\n",
    "    \t\t\tX (matrix, [n, d]): Data to be used for initialization.\n",
    "    \t\t\tP (matrix, [n, K]): The posterior probabilities for the n samples.\n",
    "\n",
    "\t\t\tReturns:\n",
    "    \t\t\tTuple (mu, Sigma, pi), \n",
    "\t\t\t\t\tmu has size\t\t[K, d]\n",
    "\t\t\t\t\tSigma has size\t[K, d, d]\n",
    "\t\t\t\t\tpi has size\t\t[K]\n",
    "\t\t\"\"\"\n",
    "\t\t# Sizes\n",
    "\t\tn, d\t= X.shape\n",
    "\t\tK\t\t= self.K\n",
    "\n",
    "\t\t# Mu \n",
    "\t\tNks\t\t= np.sum(P, axis=0, keepdims=True).T\t# (K, 1)\n",
    "\t\tP_\t\t= P.reshape((n, K, 1))\t\t\t\t\t# (n, K, 1)\n",
    "\t\tX_\t\t= X.reshape((n, 1, d))\t\t\t\t\t# (n, 1, d)\n",
    "\t\tmu_hat\t= (P_ * X_).sum(axis=0) / Nks\t\t\t# (K, d)\n",
    "\t\t\n",
    "\t\t# Sigma\n",
    "\t\tNks\t\t= Nks.reshape((K, 1, 1))\t\t\t\t# (K, 1, 1)\n",
    "\t\tP_\t\t= P_.reshape((n, K, 1, 1))\n",
    "\t\n",
    "\t\tX_\t\t= X_ - self.mu.reshape((1, K, d))\t\t# (n, K, d)\n",
    "\n",
    "\t\tX_\t\t= X_.reshape((n, K, d, 1)) * P_\n",
    "\t\tX__\t\t= X_.reshape((n, K, 1, d))\n",
    "\n",
    "\t\touter\t= X_ @ X__\t\t\t\t\t\t\t\t# (n, K, d, d)\n",
    "\t\tSi_hat\t= outer.sum(axis=0) / Nks\t\t\t\t# (K, d, d)\n",
    "\n",
    "\t\t# Pi\n",
    "\t\tpi_hat\t= Nks.squeeze() / n\n",
    "\t\treturn  mu_hat, Si_hat, pi_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the data used for the experiments to see the actual classes \n",
    "def plot_iris(X, y, title=''):\n",
    "    # Plotting\n",
    "    _, d = X.shape\n",
    "    \n",
    "    combinations = list(itertools.combinations(np.arange(d), 2))\n",
    "    \n",
    "    cols    = min(3, len(combinations) )\n",
    "    rows    = math.ceil(len(combinations)/cols)\n",
    "    fig, ax = plt.subplots(rows, cols, figsize=(4 * cols, 4 * rows))\n",
    "    \n",
    "    if len(title) > 0: fig.suptitle(title)\n",
    "    \n",
    "    # Fix indexing when there are few plots.\n",
    "    if rows == 1: ax = [ax]\n",
    "    if cols == 1: ax = [ax]\n",
    "\n",
    "    c       = 0\n",
    "    for i, j in combinations:\n",
    "        m = c // cols\n",
    "        n = c % cols\n",
    "        ax[m][n].scatter(X[:,i], X[:,j], c=y)\n",
    "        ax[m][n].set_xlabel(index_to_feature[i])\n",
    "        ax[m][n].set_ylabel(index_to_feature[j])\n",
    "        c += 1 \n",
    "    # fig.tight_layout()\n",
    "\n",
    "# Load the Iris data set\n",
    "X , y    = load_iris()\n",
    "X_, y_   = load_iris_PC()\n",
    "\n",
    "plot_iris(X, y, 'Iris')\n",
    "plot_iris(X_, y_, 'Iris 2 Principal Components')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below runs your implementation of the GMM on the simple 2D Iris data and plots it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiny experiment 2PC\n",
    "K       = 3\n",
    "gmm     = MyGMM(K)\n",
    "gmm.fit(X_, max_iter=100)\n",
    "\n",
    "plot_iris(X_, y_, 'Real labels')\n",
    "plot_iris(X_, gmm.predict(X_), \"Labels from GMM model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below runs your implementation of the GMM on the full 4D Iris data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All four dimensions iris\n",
    "K       = 3\n",
    "gmm     = MyGMM(K)\n",
    "gmm.fit(X, max_iter=100)\n",
    "plot_iris(X, y, 'Real labels')\n",
    "plot_iris(X, gmm.predict(X), \"Labels from GMM model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
